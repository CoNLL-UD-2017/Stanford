#***************************************************************
# High level stuff
[DEFAULT]
save_dir = saves/defaults
data_dir = data
LC = en
LANG = English

[Configurable]
train_files = %(data_dir)s/en-ud-train.conllu
valid_files = %(data_dir)s/en-ud-dev.conllu
verbose = True
name = None

#***************************************************************
# Vocab data structures
[Base Vocab]
cased = None
embed_size = 100

[Pretrained Vocab]
special_tokens=<PAD>:<ROOT>:<DROP>:<UNK>
skip_header = True
name = pretrained
#filename = %(data_dir)s/embeddings/%(LANG)s/%(LC)s.vectors
#filename = %(data_dir)s/embeddings/%(LANG)s/wiki.%(LC)s.vec
filename = %(data_dir)s/glove.6B/en.100d.txt
cased = False
max_rank = 500000

[Token Vocab]
name = tokens
embed_keep_prob = .5
min_occur_count = 2
max_rank = 50000

[Index Vocab]
special_tokens=<PAD>:<ROOT>

[Dep Vocab]
name = deps

[Head Vocab]
name = heads

[Word Vocab]
special_tokens=<PAD>:<ROOT>:<DROP>:<UNK>
name = words
filename = %(save_dir)s/%(name)s.txt
cased = False

[Lemma Vocab]
name = lemmas
filename = %(save_dir)s/%(name)s.txt

[Tag Vocab]
special_tokens=PAD:ROOT:DROP:UNK
name = tags
filename = %(save_dir)s/%(name)s.txt
cased = True

[X Tag Vocab]
name = xtags
filename = %(save_dir)s/%(name)s.txt

[Rel Vocab]
special_tokens=pad:root:drop:unk
name = rels
filename = %(save_dir)s/%(name)s.txt
cased = True

[Subtoken Vocab]
max_rank = 5000
n_buckets = 2
embed_model = RNNEmbed
embed_keep_prob = 1.

[Char Vocab]
special_tokens = <PAD>:<ROOT>:<DROP>:<UNK>:<META_UNK>:<START>:<STOP>
name = chars
filename = %(save_dir)s/%(name)s.txt
embed_model = CNNEmbed

[Ngram Vocab]
special_tokens = <PAD>:<ROOT>:<DROP>:<UNK>:<META_UNK>
name = ngrams
filename = %(save_dir)s/%(name)s.txt
embed_model = MLPEmbed

[Ngram Multivocab]
special_tokens = <PAD>:<ROOT>:<DROP>:<UNK>:<META_UNK>
name = multi-ngram
max_n = 5
embed_model = MLPEmbed

[Bytepair Vocab]
name = bytepairs
filename = %(save_dir)s/%(name)s.txt
n_bytepairs = 500
embed_model = MLPEmbed

[Multivocab]
embed_keep_prob = .5

#***************************************************************
# Vocab subtoken embedding models
[Base Embed]

[MLP Embed]
mlp_size = 300
mlp_keep_prob = .5
mlp_func = leaky_relu

[RNN Embed]
recur_cell = LSTMCell
rnn_func = birnn
recur_keep_prob = .5
ff_keep_prob = .5

[CNN Embed]
conv_size = 300
window_size = 6
conv_keep_prob = .5
conv_func = leaky_relu

#***************************************************************
# Sequence data structures
[Multibucket]
n_buckets = 2
name = multibucket

[Bucket]
name = None

[Trainset]
name = trainset
data_files = train_files
n_buckets = 40
batch_by = tokens
batch_size = 2500

[Validset]
name = validset
data_files = valid_files
n_buckets = 10
batch_by = tokens
batch_size = 50000

[Testset]
name = testset
data_files = test_files
n_buckets = 10
batch_by = tokens
batch_size = 50000

#***************************************************************
# Parser models
[NN]
mlp_keep_prob = .5

[Base Cell]
forget_bias = 0
recur_func = tanh
recur_size = 300

[RNN Cell]
recur_func = leaky_relu

[Base Parser]
input_vocabs = words:tags
output_vocabs = rels:heads
recur_cell = LSTMCell
n_layers = 3
mlp_func = leaky_relu
# TODO make sure you add this to Base Cell
recur_size = 300
arc_mlp_size = 500
rel_mlp_size = 100
rnn_func = birnn
recur_keep_prob = .5
ff_keep_prob = .5

[Parser]
name = parser

[Joint Parser]
mlp_func = leaky_relu
tag_mlp_size = 500
arc_mlp_size = 500
rel_mlp_size = 100

#***************************************************************
# Training 
[Network]
name = network
subtoken_vocab = CharVocab
parser_model = Parser
max_train_iters = 50000
validate_every = 100
save_every = 1
per_process_gpu_memory_fraction = -1

#***************************************************************
# Miscellaneous
[Radam Optimizer]
name = radam
# TODO keep adjusting lr?
learning_rate = 2e-3
decay = .75
decay_steps = 5000
clip = 5
mu = .9
nu = .9
gamma = 0
chi = 0
epsilon = 1e-12

[Zipf]
n_zipfs = 3
name = zipf
filename = %(save_dir)s/%(name)s.txt
batch_size = 500
max_train_iters = 5000
print_every = 500

[Bucketer]
name = bucketer
filename = %(save_dir)s/%(name)s.txt
